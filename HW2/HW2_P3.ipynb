{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.platform import gfile\n",
    "import glob\n",
    "from PIL import Image\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import correlation\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from skimage import color\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images array:  (8973, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "image_glob = glob.glob('./Images/*.JPEG')\n",
    "\n",
    "# Resizing the images to a smaller size for faster computation and removing grayscale\n",
    "images = np.array([np.array(Image.open(i).resize((64,64))) for i in image_glob if len(np.array(Image.open(i).resize((64,64))).shape) == 3 ])\n",
    "\n",
    "print(\"Shape of images array: \", images.shape)\n",
    "\n",
    "# Scaling the images\n",
    "images = images.astype('float32') / 255\n",
    "\n",
    "img_len = [1241,1569,1271,1883,1802,1207]\n",
    "\n",
    "# Creating the labels for the images\n",
    "Y_label = labels = np.concatenate((np.repeat(0, img_len[0]), np.repeat(1, img_len[1]), np.repeat(2, img_len[2]), np.repeat(3,img_len[3]), np.repeat(4,img_len[4]), np.repeat(5,img_len[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8973/8973 [==============================] - 113s - loss: 0.6290   \n",
      "Epoch 2/10\n",
      "8973/8973 [==============================] - 111s - loss: 0.5759   \n",
      "Epoch 3/10\n",
      "8973/8973 [==============================] - 110s - loss: 0.5679   \n",
      "Epoch 4/10\n",
      "8973/8973 [==============================] - 116s - loss: 0.5601   \n",
      "Epoch 5/10\n",
      "8973/8973 [==============================] - 115s - loss: 0.5561   \n",
      "Epoch 6/10\n",
      "8973/8973 [==============================] - 118s - loss: 0.5539   \n",
      "Epoch 7/10\n",
      "8973/8973 [==============================] - 111s - loss: 0.5525   \n",
      "Epoch 8/10\n",
      "8973/8973 [==============================] - 108s - loss: 0.5512   \n",
      "Epoch 9/10\n",
      "8973/8973 [==============================] - 779s - loss: 0.5501   \n",
      "Epoch 10/10\n",
      "8973/8973 [==============================] - 110s - loss: 0.5487   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e500e4fa90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will train the autoencoder on the whole dataset for better performance and make the train, test splits after training\n",
    "\n",
    "input_img = Input(shape=(64, 64, 3)) #downsized  \n",
    "\n",
    "#Encoder\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "#Decoder\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "encoder = Model(input_img, encoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(images, images,\n",
    "                epochs = 10,\n",
    "                batch_size = 128,\n",
    "                shuffle = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN configuration used:\n",
    "#### Encoder: 3 convolution layers and 2 maxpooling layers. Filter size of convolution layers = 3x3\n",
    "#### Decoder: 3 convolution layers and 2 Upsampling layers. Filter size of convolution layers = 3x3\n",
    "#### Epochs: 10 ; batch size: 128 ; activation: relu\n",
    "#### The number of CNN layers was increased and it gave better performance. Also, the optimizer used was Adam, as it gave a better result in the previous question. We can't increase filter size because it slows the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining Functions to use in prediction metrics\n",
    "\n",
    "def Euclidean_Distance(X_Train, X_test, Y, row, k):\n",
    "    # Making an empty distance list\n",
    "    dist = []\n",
    "    rows = X_Train.shape[0] \n",
    "    for i in range(0, rows):\n",
    "        dist.append([i, np.linalg.norm(X_Train[i,:] - X_test[row,:])])\n",
    "    \n",
    "    dist.sort(key = lambda x:x[1])\n",
    "    opt_k = [dist[i][0] for i in range(0,k)]\n",
    "    predicted =  np.bincount(Y[opt_k]).argmax()\n",
    "    \n",
    "    return predicted\n",
    "\n",
    "\n",
    "def Pearson_Distance(X_Train, X_test, Y, row, k):\n",
    "    # Making an empty distance list\n",
    "    dist = []\n",
    "    rows = X_Train.shape[0]\n",
    "    for i in range(0, rows):\n",
    "        dist.append([i,correlation(X_Train[i,:],X_test[row,:])])\n",
    "    \n",
    "    dist.sort(key = lambda x:x[1])\n",
    "    opt_k = [dist[i][0] for i in range(0,k)]\n",
    "    predicted =  np.bincount(Y[opt_k]).argmax()\n",
    "    \n",
    "    return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions Matrices, Accuracy for autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the predictions using the encoder on the whole dataset to get the Compressed Images\n",
    "X_comp = (encoder.predict(images))\n",
    "X_comp = X_comp.reshape(len(X_comp), np.prod(X_comp.shape[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy Using Eucledian Distance Autoencoder =  0.394986072423\n",
      "Confusion Matrix Using Eucledian Distance Autoencoder\n",
      "[[ 54  60  12  20  96  10]\n",
      " [ 14 110  25  13 129  14]\n",
      " [  8  44 115  11  54  14]\n",
      " [ 22  87  27  65 154   6]\n",
      " [ 18  36   9   8 310   2]\n",
      " [ 12  36  53  13  79  55]]\n",
      "\n",
      "\n",
      " Accuracy Using Pearson Distance Autoencoder =  0.392757660167\n",
      "Confusion Matrix Using Pearson Distance Autoencoder\n",
      "[[ 71  49   7  21  93  11]\n",
      " [ 22  96  27  23 119  18]\n",
      " [  6  46 103  12  60  19]\n",
      " [ 30  69  26  68 159   9]\n",
      " [ 14  38  16   7 304   4]\n",
      " [ 11  40  45  10  79  63]]\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder train & test split\n",
    "X_train, X_Test, Y_Train, Y_Test = train_test_split(X_comp, Y_label, test_size=0.2, random_state=0)\n",
    "\n",
    "Y_Pred_E = np.zeros(shape=(len(Y_Test),))\n",
    "Y_Pred_P = np.zeros(shape=(len(Y_Test),))\n",
    "\n",
    "for i in range(0,len(Y_Pred_E)):\n",
    "    Y_Pred_E[i,] = Euclidean_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "    Y_Pred_P[i,] = Pearson_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "\n",
    "conf1 = confusion_matrix(Y_Test, Y_Pred_E)\n",
    "conf2 = confusion_matrix(Y_Test, Y_Pred_P)\n",
    "acc1 = np.trace(conf1) / len(Y_Test)\n",
    "acc2 = np.trace(conf2) / len(Y_Test)\n",
    "\n",
    "print(\"\\n\\nAccuracy Using Eucledian Distance Autoencoder = \",acc1)\n",
    "print(\"Confusion Matrix Using Eucledian Distance Autoencoder\")\n",
    "print(conf1)\n",
    "\n",
    "print(\"\\n\\n Accuracy Using Pearson Distance Autoencoder = \",acc2)\n",
    "print(\"Confusion Matrix Using Pearson Distance Autoencoder\")\n",
    "print(conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_SVD = images.reshape(len(images),np.prod(images.shape[1:]))\n",
    "Model_SVD = TruncatedSVD(n_components = 300, n_iter=25, random_state=0) # Tried at intervals 100, 200, 300. It stabilizes after 200. \n",
    "X_SVD = Model_SVD.fit_transform(X_SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions Matrices, Accuracy for SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy Using Eucledian Distance SVD =  0.380501392758\n",
      "Confusion Matrix Using Eucledian Distance SVD\n",
      "[[ 60  59  14  20  86  13]\n",
      " [ 15 108  43  26 100  13]\n",
      " [ 13  66  89  20  44  14]\n",
      " [ 20 101  26  76 135   3]\n",
      " [ 15  34  13  17 300   4]\n",
      " [ 18  55  45  17  63  50]]\n",
      "\n",
      " Accuracy Using Pearson Distance SVD =  0.37938718663\n",
      "Confusion Matrix Using Pearson Distance SVD\n",
      "[[ 60  67  14  16  87   8]\n",
      " [ 30 109  40  22  96   8]\n",
      " [ 17  63  92  14  49  11]\n",
      " [ 49  73  26  77 133   3]\n",
      " [ 25  28  15  16 296   3]\n",
      " [ 13  51  47  18  72  47]]\n"
     ]
    }
   ],
   "source": [
    "# Splitting data for SVD\n",
    "X_train, X_Test, Y_Train, Y_Test = train_test_split(X_SVD, Y_label, test_size=0.2, random_state=0)\n",
    "\n",
    "Y_Pred_E = np.zeros(shape=(len(Y_Test),))\n",
    "Y_Pred_P = np.zeros(shape=(len(Y_Test),))\n",
    "\n",
    "for i in range(0,len(Y_Pred_E)):\n",
    "    Y_Pred_E[i,] = Euclidean_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "    Y_Pred_P[i,] = Pearson_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "\n",
    "conf1 = confusion_matrix(Y_Test, Y_Pred_E)\n",
    "conf2 = confusion_matrix(Y_Test, Y_Pred_P)\n",
    "acc1 = np.trace(conf1) / len(Y_Test)\n",
    "acc2 = np.trace(conf2) / len(Y_Test)\n",
    "\n",
    "print(\"\\n Accuracy Using Eucledian Distance SVD = \", acc1)\n",
    "print(\"Confusion Matrix Using Eucledian Distance SVD\")\n",
    "print(conf1)\n",
    "\n",
    "print(\"\\n Accuracy Using Pearson Distance SVD = \", acc2)\n",
    "print(\"Confusion Matrix Using Pearson Distance SVD\")\n",
    "print(conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_RGB = np.zeros(shape=(len(images),256*3))\n",
    "\n",
    "for row in range(0,len(images)):\n",
    "    r_hist, _ = np.histogram(images[row,:,:,0], bins = 256)\n",
    "    g_hist, _ = np.histogram(images[row,:,:,1], bins = 256)\n",
    "    b_hist, _ = np.histogram(images[row,:,:,2], bins = 256)\n",
    "    arr = np.concatenate((r_hist,g_hist,b_hist),axis=0)\n",
    "    X_RGB[row,:] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions Matrices, Accuracy for RGB Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy Using Eucledian Distance RGB Histogram =  0.377158774373\n",
      "Confusion Matrix Using Eucledian Distance RGB Histogram\n",
      "[[ 95  48  16  21  17  55]\n",
      " [ 42 102  50  21  36  54]\n",
      " [ 17  44 108  12  13  52]\n",
      " [ 48  67  29 124  28  65]\n",
      " [ 39  86  32  33 139  54]\n",
      " [ 25  44  47   5  18 109]]\n",
      "\n",
      " Accuracy Using Pearson Distance RGB Histogram =  0.404456824513\n",
      "Confusion Matrix Using Pearson Distance RGB Histogram\n",
      "[[118  50  16  26  21  21]\n",
      " [ 43  98  45  28  45  46]\n",
      " [ 20  44  98  13  32  39]\n",
      " [ 49  68  26 150  32  36]\n",
      " [ 44  71  22  26 185  35]\n",
      " [ 33  43  42  15  38  77]]\n"
     ]
    }
   ],
   "source": [
    "## RGB Histogram train & test split\n",
    "X_train, X_Test, Y_Train, Y_Test = train_test_split(X_RGB, Y_label, test_size=0.2, random_state=0)\n",
    "\n",
    "Y_Pred_E = np.zeros(shape=(len(Y_Test),))\n",
    "Y_Pred_P = np.zeros(shape=(len(Y_Test),))\n",
    "\n",
    "for i in range(0,len(Y_Pred_E)):\n",
    "    Y_Pred_E[i,] = Euclidean_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "    Y_Pred_P[i,] = Pearson_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "\n",
    "conf1 = confusion_matrix(Y_Test, Y_Pred_E)\n",
    "conf2 = confusion_matrix(Y_Test, Y_Pred_P)\n",
    "acc1 = np.trace(conf1) / len(Y_Test)\n",
    "acc2 = np.trace(conf2) / len(Y_Test)\n",
    "\n",
    "print(\"\\n Accuracy Using Eucledian Distance RGB Histogram = \",acc1)\n",
    "print(\"Confusion Matrix Using Eucledian Distance RGB Histogram\")\n",
    "print(conf1)\n",
    "\n",
    "print(\"\\n Accuracy Using Pearson Distance RGB Histogram = \",acc2)\n",
    "print(\"Confusion Matrix Using Pearson Distance RGB Histogram\")\n",
    "print(conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_HSV = np.zeros(shape=(len(images),692))\n",
    "\n",
    "for row in range(0,len(images)):\n",
    "    img = images[row,:,:,:]\n",
    "    img_hsv = color.rgb2hsv(img)\n",
    "    h_hist, _ = np.histogram(img_hsv[:,:,0], bins = 180 )\n",
    "    s_hist, _ = np.histogram(img_hsv[:,:,1], bins = 256 )\n",
    "    v_hist, _ = np.histogram(img_hsv[:,:,2], bins = 256 )\n",
    "    arr = np.concatenate((h_hist,s_hist,v_hist), axis=0)\n",
    "    X_HSV[row,:] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions Matrices, Accuracy for HSV Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy Using Eucledian Distance HSV Histogram 0.413370473538\n",
      "Confusion Matrix Using Eucledian Distance HSV Histogram\n",
      "[[ 97  29  15  27  24  60]\n",
      " [ 22  78  53  24  56  72]\n",
      " [  9  29 150   4   9  45]\n",
      " [ 37  44  37 139  24  80]\n",
      " [ 27  74  41  48 146  47]\n",
      " [ 12  44  37   6  17 132]]\n",
      "\n",
      " Accuracy Using Pearson Distance HSV Histogram =  0.421727019499\n",
      "Confusion Matrix Using Pearson Distance HSV Histogram\n",
      "[[ 97  32  18  31  28  46]\n",
      " [ 31  81  54  19  70  50]\n",
      " [ 10  33 145   9  23  26]\n",
      " [ 34  64  30 136  45  52]\n",
      " [ 39  77  25  29 182  31]\n",
      " [ 19  44  33   9  27 116]]\n"
     ]
    }
   ],
   "source": [
    "# HSV Histogram train & test split\n",
    "X_train, X_Test, Y_Train, Y_Test = train_test_split(X_HSV, Y_label, test_size=0.2, random_state=0)\n",
    "\n",
    "Y_Pred_E = np.zeros(shape=(len(Y_Test),))\n",
    "Y_Pred_P = np.zeros(shape=(len(Y_Test),))\n",
    "\n",
    "for i in range(0,len(Y_Pred_E)):\n",
    "    Y_Pred_E[i,] = Euclidean_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "    Y_Pred_P[i,] = Pearson_Distance(X_train , X_Test, Y_Train , i, 5)\n",
    "\n",
    "conf1 = confusion_matrix(Y_Test, Y_Pred_E)\n",
    "conf2 = confusion_matrix(Y_Test, Y_Pred_P)\n",
    "acc1 = np.trace(conf1) / len(Y_Test)\n",
    "acc2 = np.trace(conf2) / len(Y_Test)\n",
    "\n",
    "print(\"\\n Accuracy Using Eucledian Distance HSV Histogram\", acc1)\n",
    "print(\"Confusion Matrix Using Eucledian Distance HSV Histogram\")\n",
    "print(conf1)\n",
    "\n",
    "print(\"\\n Accuracy Using Pearson Distance HSV Histogram = \", acc2)\n",
    "print(\"Confusion Matrix Using Pearson Distance HSV Histogram\")\n",
    "print(conf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the results between different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Among the 4 models, HSV histogram gives the max accuracy of 42.1%. This is probably because the histogram method doesn't reduce dimensionality that much and utilizes most amount of information.\n",
    "#### The Singular Value Decomposition method performs the worst.\n",
    "#### The Pearson Correlation method gives better accuracy compared to using Eucliean Distance for all the classifiers.\n",
    "#### We could improve the autoencoder by further tuning it. It performs better than the other two methods apart from HSV."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
